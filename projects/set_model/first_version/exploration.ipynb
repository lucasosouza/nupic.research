{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg19_bn\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "data_dir = os.path.expanduser(\"~/nta/results\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# fix a random seed\n",
    "torch.manual_seed(32)\n",
    "\n",
    "# set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run regular MLP on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'FashionMNIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2860164866727941 0.35302426207299326\n"
     ]
    }
   ],
   "source": [
    "# check mean and std\n",
    "tempset = getattr(datasets, dataset)(root=data_dir, train=True,\n",
    "                                     transform=transforms.ToTensor(), download=True)\n",
    "datastats_mean = tempset.train_data.float().mean().item()/255\n",
    "datastats_std = tempset.train_data.float().std().item()/255\n",
    "print(datastats_mean, datastats_std)\n",
    "del tempset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.1307,), (0.3081,)), #MNIST\n",
    "        transforms.Normalize((datastats_mean,), (datastats_std,)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = getattr(datasets, dataset)(root=data_dir, train=True, transform=transform, \n",
    "                                       download=True)\n",
    "test_set = getattr(datasets, dataset)(root=data_dir, train=False, transform=transform, \n",
    "                                      download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.train_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0932364940>"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = train_set.train_data[np.random.randint(60000)].numpy()\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 128\n",
    "batch_size_test = 128\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size_train, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=784, num_classes=10, init_weights=True,\n",
    "                 hidden_sizes = [1000, 1000, 1000], device=device):\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        self.device = device\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.input_size, hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[2], num_classes)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x.view(-1, self.input_size).to(self.device))\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(device=device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loss_func, loader, optimizer=None, train=True):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    num_samples = 0\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        # get inputs and label\n",
    "        inputs, targets = data\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # zero gradients\n",
    "        if train: optimizer.zero_grad()\n",
    "                \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        correct += (targets==torch.max(outputs, dim=1)[1]).sum().item()\n",
    "        loss = loss_func(outputs, targets)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # keep track of loss\n",
    "        epoch_loss += loss.item()\n",
    "        num_samples += inputs.shape[0]\n",
    "        \n",
    "    return (epoch_loss / num_samples) * 1000, correct/num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(epochs=20):\n",
    "    max_train_acc = 0\n",
    "    max_val_acc = 0\n",
    "    for epoch in range(epochs):    \n",
    "        model.train()\n",
    "        train_loss, train_acc = run_epoch(model, loss_func, train_loader, optimizer, train=True)\n",
    "        max_train_acc = max(train_acc, max_train_acc)\n",
    "        model.eval()\n",
    "        val_loss, val_acc     = run_epoch(model, loss_func, test_loader, train=False)\n",
    "        max_val_acc = max(val_acc, max_val_acc)        \n",
    "        print('Epoch: %d Train loss: %.4f Train Acc: %.4f Val loss: %.4f Val Acc: %.4f' % \n",
    "              (epoch + 1, train_loss, train_acc, val_loss, val_acc))\n",
    "    print('\\nFinal - Best Train Acc: %.4f Best Val acc: %.4f' % (max_train_acc, max_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 5.1337 Train Acc: 0.7699 Val loss: 3.8193 Val Acc: 0.8230\n",
      "Epoch: 2 Train loss: 3.2550 Train Acc: 0.8486 Val loss: 3.2947 Val Acc: 0.8466\n",
      "Epoch: 3 Train loss: 2.8686 Train Acc: 0.8652 Val loss: 3.1101 Val Acc: 0.8545\n",
      "Epoch: 4 Train loss: 2.6161 Train Acc: 0.8771 Val loss: 2.8331 Val Acc: 0.8686\n",
      "Epoch: 5 Train loss: 2.4346 Train Acc: 0.8852 Val loss: 2.7914 Val Acc: 0.8739\n",
      "Epoch: 6 Train loss: 2.2850 Train Acc: 0.8910 Val loss: 2.7238 Val Acc: 0.8762\n",
      "Epoch: 7 Train loss: 2.1571 Train Acc: 0.8985 Val loss: 2.5857 Val Acc: 0.8825\n",
      "Epoch: 8 Train loss: 2.0290 Train Acc: 0.9032 Val loss: 2.6326 Val Acc: 0.8796\n",
      "Epoch: 9 Train loss: 1.9257 Train Acc: 0.9095 Val loss: 2.5624 Val Acc: 0.8803\n",
      "Epoch: 10 Train loss: 1.8362 Train Acc: 0.9138 Val loss: 2.4978 Val Acc: 0.8843\n",
      "Epoch: 11 Train loss: 1.7460 Train Acc: 0.9177 Val loss: 2.5304 Val Acc: 0.8833\n",
      "Epoch: 12 Train loss: 1.6392 Train Acc: 0.9229 Val loss: 2.5593 Val Acc: 0.8878\n",
      "Epoch: 13 Train loss: 1.5546 Train Acc: 0.9270 Val loss: 2.5160 Val Acc: 0.8895\n",
      "Epoch: 14 Train loss: 1.4647 Train Acc: 0.9316 Val loss: 2.5378 Val Acc: 0.8919\n",
      "Epoch: 15 Train loss: 1.3877 Train Acc: 0.9346 Val loss: 2.5777 Val Acc: 0.8931\n",
      "Epoch: 16 Train loss: 1.3092 Train Acc: 0.9380 Val loss: 2.5967 Val Acc: 0.8911\n",
      "Epoch: 17 Train loss: 1.2265 Train Acc: 0.9415 Val loss: 2.8170 Val Acc: 0.8891\n",
      "Epoch: 18 Train loss: 1.1489 Train Acc: 0.9460 Val loss: 2.6787 Val Acc: 0.8895\n",
      "Epoch: 19 Train loss: 1.1013 Train Acc: 0.9480 Val loss: 2.7705 Val Acc: 0.8894\n",
      "Epoch: 20 Train loss: 1.0127 Train Acc: 0.9518 Val loss: 2.7678 Val Acc: 0.8930\n",
      "\n",
      "Final - Best Train Acc: 0.9518 Best Val acc: 0.8931\n",
      "CPU times: user 4min 26s, sys: 396 ms, total: 4min 27s\n",
      "Wall time: 4min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an improved model of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added: relu, batchnorm, dropout at second to last layer, weight decay\n",
    "def new_init(self, input_size=784, num_classes=10, init_weights=True,\n",
    "             hidden_sizes = [1000, 1000, 1000], device=device):\n",
    "\n",
    "    super(MLP, self).__init__()\n",
    "    self.device = device\n",
    "    self.input_size = input_size\n",
    "\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Linear(self.input_size, hidden_sizes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(hidden_sizes[0]),\n",
    "        nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(hidden_sizes[1]),\n",
    "        nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "        nn.Dropout(p=0.3),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(hidden_sizes[2]),\n",
    "        nn.Linear(hidden_sizes[2], num_classes)\n",
    "    ).to(self.device)\n",
    "\n",
    "    if init_weights:\n",
    "        self._initialize_weights()\n",
    "        \n",
    "MLP.__init__ = new_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to SGD with weight decay\n",
    "model = MLP(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, weight_decay=2e-4, momentum=0.9)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 3.5526 Train Acc: 0.8385 Val loss: 3.0888 Val Acc: 0.8567\n",
      "Epoch: 2 Train loss: 2.6959 Train Acc: 0.8710 Val loss: 2.9206 Val Acc: 0.8620\n",
      "Epoch: 3 Train loss: 2.4448 Train Acc: 0.8837 Val loss: 2.7977 Val Acc: 0.8674\n",
      "Epoch: 4 Train loss: 2.2292 Train Acc: 0.8946 Val loss: 2.8411 Val Acc: 0.8692\n",
      "Epoch: 5 Train loss: 2.1174 Train Acc: 0.8983 Val loss: 2.6486 Val Acc: 0.8808\n",
      "Epoch: 6 Train loss: 2.0019 Train Acc: 0.9031 Val loss: 2.6650 Val Acc: 0.8796\n",
      "Epoch: 7 Train loss: 1.8990 Train Acc: 0.9078 Val loss: 2.5938 Val Acc: 0.8804\n",
      "Epoch: 8 Train loss: 1.8067 Train Acc: 0.9133 Val loss: 2.7324 Val Acc: 0.8779\n",
      "Epoch: 9 Train loss: 1.7261 Train Acc: 0.9174 Val loss: 2.6742 Val Acc: 0.8830\n",
      "Epoch: 10 Train loss: 1.6300 Train Acc: 0.9219 Val loss: 2.5833 Val Acc: 0.8843\n",
      "Epoch: 11 Train loss: 1.5823 Train Acc: 0.9237 Val loss: 2.7901 Val Acc: 0.8837\n",
      "Epoch: 12 Train loss: 1.5265 Train Acc: 0.9257 Val loss: 2.7427 Val Acc: 0.8864\n",
      "Epoch: 13 Train loss: 1.4568 Train Acc: 0.9292 Val loss: 2.8171 Val Acc: 0.8804\n",
      "Epoch: 14 Train loss: 1.4026 Train Acc: 0.9312 Val loss: 2.6902 Val Acc: 0.8934\n",
      "Epoch: 15 Train loss: 1.3495 Train Acc: 0.9337 Val loss: 2.6501 Val Acc: 0.8955\n",
      "Epoch: 16 Train loss: 1.2869 Train Acc: 0.9372 Val loss: 2.6788 Val Acc: 0.8949\n",
      "Epoch: 17 Train loss: 1.2296 Train Acc: 0.9398 Val loss: 2.7345 Val Acc: 0.8891\n",
      "Epoch: 18 Train loss: 1.2112 Train Acc: 0.9416 Val loss: 2.6742 Val Acc: 0.8937\n",
      "Epoch: 19 Train loss: 1.1424 Train Acc: 0.9445 Val loss: 2.8298 Val Acc: 0.8925\n",
      "Epoch: 20 Train loss: 1.1240 Train Acc: 0.9442 Val loss: 2.9470 Val Acc: 0.8880\n",
      "\n",
      "Final - Best Train Acc: 0.9445 Best Val acc: 0.8955\n",
      "CPU times: user 4min 29s, sys: 388 ms, total: 4min 30s\n",
      "Wall time: 4min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Best results on MNIST after 80 epochs are 0.9885 and final 0.9876, better than reported in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement fixed sparsity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Calculate sparsity - why last layer is not possible\n",
    "epsilon=20\n",
    "shape = (10,100)\n",
    "sparsity = epsilon * (shape[0]+shape[1])/(shape[0]*shape[1])\n",
    "sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added: relu, batchnorm, dropout at second to last layer, weight decay\n",
    "def new_init(self, input_size=784, num_classes=10, init_weights=True,\n",
    "             hidden_sizes = [1000, 1000, 1000], device=device,\n",
    "             epsilon=20, bias=False):\n",
    "\n",
    "    super(MLP, self).__init__()\n",
    "    self.device = device\n",
    "    self.input_size = input_size\n",
    "           \n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Linear(self.input_size, hidden_sizes[0], bias=bias),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(hidden_sizes[0]),\n",
    "        nn.Linear(hidden_sizes[0], hidden_sizes[1], bias=bias),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(hidden_sizes[1]),\n",
    "        nn.Linear(hidden_sizes[1], hidden_sizes[2], bias=bias),\n",
    "        nn.Dropout(p=0.3),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(hidden_sizes[2]),\n",
    "        nn.Linear(hidden_sizes[2], num_classes, bias=bias)\n",
    "    ).to(self.device)\n",
    "\n",
    "    # calculate sparsity masks\n",
    "    self.masks = []\n",
    "    linear_layers = [m for m in self.modules() if isinstance(m, nn.Linear)]\n",
    "    for i, layer in enumerate(linear_layers,1):\n",
    "        shape = layer.weight.shape\n",
    "        sparsity = (shape[0]+shape[1])/(shape[0]*shape[1])\n",
    "        # at last layer, can't apply epsilon =20 - see calculations\n",
    "        if i != len(linear_layers): sparsity *= epsilon\n",
    "        mask = torch.rand(shape) < sparsity\n",
    "        self.masks.append(mask.float().to(self.device))\n",
    "    \n",
    "    if init_weights:\n",
    "        self._initialize_weights()\n",
    "\n",
    "def new_initialize_weights(self):\n",
    "              \n",
    "    masks = iter(self.masks)\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "        elif isinstance(m, nn.Linear):           \n",
    "            with torch.no_grad():\n",
    "                tensor = (torch.randn(m.weight.shape) * 1e-2).to(self.device)\n",
    "                tensor = tensor * next(masks)\n",
    "            m.weight = torch.nn.Parameter(tensor)            \n",
    "\n",
    "MLP.__init__ = new_init\n",
    "MLP._initialize_weights = new_initialize_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to SGD with weight decay\n",
    "model = MLP(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, weight_decay=2e-4, momentum=0.9)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loss_func, loader, optimizer=None, train=True):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    num_samples = 0\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        # get inputs and label\n",
    "        inputs, targets = data\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # zero gradients\n",
    "        if train: optimizer.zero_grad()\n",
    "                \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        correct += (targets==torch.max(outputs, dim=1)[1]).sum().item()\n",
    "        loss = loss_func(outputs, targets)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            # zero the gradients in the dead connections\n",
    "            masks = iter(model.masks)\n",
    "            for m in model.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    m.weight.grad *= next(masks)\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "\n",
    "        # keep track of loss\n",
    "        epoch_loss += loss.item()\n",
    "        num_samples += inputs.shape[0]\n",
    "        \n",
    "    return (epoch_loss / num_samples) * 1000, correct/num_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 3.7621 Train Acc: 0.8328 Val loss: 2.9747 Val Acc: 0.8651\n",
      "Epoch: 2 Train loss: 2.7088 Train Acc: 0.8745 Val loss: 2.7837 Val Acc: 0.8693\n",
      "Epoch: 3 Train loss: 2.4467 Train Acc: 0.8849 Val loss: 2.6173 Val Acc: 0.8760\n",
      "Epoch: 4 Train loss: 2.2475 Train Acc: 0.8932 Val loss: 2.5732 Val Acc: 0.8807\n",
      "Epoch: 5 Train loss: 2.0787 Train Acc: 0.9016 Val loss: 2.6679 Val Acc: 0.8780\n",
      "Epoch: 6 Train loss: 1.9581 Train Acc: 0.9065 Val loss: 2.6185 Val Acc: 0.8844\n",
      "Epoch: 7 Train loss: 1.8417 Train Acc: 0.9122 Val loss: 2.5329 Val Acc: 0.8848\n",
      "Epoch: 8 Train loss: 1.7650 Train Acc: 0.9163 Val loss: 2.4876 Val Acc: 0.8867\n",
      "Epoch: 9 Train loss: 1.6626 Train Acc: 0.9214 Val loss: 2.5397 Val Acc: 0.8839\n",
      "Epoch: 10 Train loss: 1.6090 Train Acc: 0.9230 Val loss: 2.4845 Val Acc: 0.8865\n",
      "Epoch: 11 Train loss: 1.5298 Train Acc: 0.9266 Val loss: 2.5219 Val Acc: 0.8896\n",
      "Epoch: 12 Train loss: 1.4554 Train Acc: 0.9296 Val loss: 2.5165 Val Acc: 0.8846\n",
      "Epoch: 13 Train loss: 1.3989 Train Acc: 0.9333 Val loss: 2.4738 Val Acc: 0.8928\n",
      "Epoch: 14 Train loss: 1.3483 Train Acc: 0.9359 Val loss: 2.5537 Val Acc: 0.8918\n",
      "Epoch: 15 Train loss: 1.2876 Train Acc: 0.9387 Val loss: 2.5572 Val Acc: 0.8917\n",
      "Epoch: 16 Train loss: 1.2515 Train Acc: 0.9405 Val loss: 2.5459 Val Acc: 0.8941\n",
      "Epoch: 17 Train loss: 1.1900 Train Acc: 0.9432 Val loss: 2.6061 Val Acc: 0.8936\n",
      "Epoch: 18 Train loss: 1.1529 Train Acc: 0.9442 Val loss: 2.5742 Val Acc: 0.8917\n",
      "Epoch: 19 Train loss: 1.1126 Train Acc: 0.9459 Val loss: 2.5790 Val Acc: 0.8944\n",
      "Epoch: 20 Train loss: 1.0836 Train Acc: 0.9477 Val loss: 2.6660 Val Acc: 0.8926\n",
      "\n",
      "Final - Best Train Acc: 0.9477 Best Val acc: 0.8944\n",
      "CPU times: user 4min 25s, sys: 408 ms, total: 4min 26s\n",
      "Wall time: 4min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.045239795918367354\n",
      "0.03987799999999997\n",
      "0.039995\n",
      "0.10340000000000005\n"
     ]
    }
   ],
   "source": [
    "# Test: sparsity holds at the end of training\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Linear):\n",
    "        zeros = torch.sum((m.weight == 0).int()).item()\n",
    "        size = np.prod(m.weight.shape)\n",
    "        print(1- zeros/size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't seem to loose a lot of performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement SET sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the magnitude treshold\n",
    "def non_zero(tensor):\n",
    "    \"\"\" Support function. Accepts a tensor, returns a tensor with only non-zero values \"\"\"\n",
    "    return tensor.view(-1)[tensor.view(-1).nonzero()].view(-1)        \n",
    "        \n",
    "def sep_update(self, A, M, zeta=0.3):\n",
    "        \"\"\" Calculate new weights based on SEP approach \"\"\"\n",
    "\n",
    "        shape = A.shape\n",
    "        n_total = np.prod(shape)\n",
    "        n_zeros = torch.sum(A == 0).item()\n",
    "        n_values = n_total - n_zeros\n",
    "        sparsity = n_zeros / n_total\n",
    "\n",
    "        A_abs = torch.abs(A)\n",
    "        threshold, _ = torch.kthvalue(non_zero(A_abs), int(zeta*(n_values)))\n",
    "        N = A_abs > threshold.item()\n",
    "        A_prime = A * N.float()\n",
    "\n",
    "        # extract mean and std of current weights\n",
    "        # if batch norm, can also get from the batch norm layer the running mean and std?\n",
    "        A_nonzero = non_zero(A)\n",
    "        prev_mean = torch.mean(A_nonzero)\n",
    "        prev_std = torch.std(A_nonzero)\n",
    "\n",
    "        # initialize matrix of random values \n",
    "        R = torch.randn(shape).to(self.device) * prev_std + prev_mean \n",
    "        # zero out values where there were previous weights\n",
    "        R_prime = R * (M == 0).float()\n",
    "        # decide which matrices to keep\n",
    "        p_update = zeta / (1-sparsity)\n",
    "        P = torch.rand(shape).to(self.device) < p_update\n",
    "        # only keep weights selected\n",
    "        R_double_prime = R_prime * P.float()\n",
    "\n",
    "        # get final matrix \n",
    "        return (A_prime + R_double_prime).to(self.device)    \n",
    "    \n",
    "def reinitialize_weights(self):        \n",
    "    \"\"\" Reinitialize weights \"\"\"\n",
    "\n",
    "    zeta = 0.3\n",
    "    masks = enumerate(self.masks, 0)\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, nn.Linear):           \n",
    "            mask_idx, mask = next(masks)\n",
    "            new_weights = self.sep_update(m.weight, mask, zeta)\n",
    "            self.masks[mask_idx] = (new_weights > 0).float().to(self.device)\n",
    "        \n",
    "MLP.sep_update = sep_update\n",
    "MLP.reinitialize_weights = reinitialize_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to SGD with weight decay\n",
    "model = MLP(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, weight_decay=2e-4, momentum=0.9)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loss_func, loader, optimizer=None, train=True):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    num_samples = 0\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        # get inputs and label\n",
    "        inputs, targets = data\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # zero gradients\n",
    "        if train: optimizer.zero_grad()\n",
    "                \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        correct += (targets==torch.max(outputs, dim=1)[1]).sum().item()\n",
    "        loss = loss_func(outputs, targets)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            # zero the gradients in the dead connections\n",
    "            masks = iter(model.masks)\n",
    "            for m in model.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    m.weight.grad *= next(masks)\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "            \n",
    "        # keep track of loss\n",
    "        epoch_loss += loss.item()\n",
    "        num_samples += inputs.shape[0]\n",
    "        \n",
    "    # at the end of epoch, reinitialize weights\n",
    "    model.reinitialize_weights()\n",
    "        \n",
    "    return (epoch_loss / num_samples) * 1000, correct/num_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 3.7417 Train Acc: 0.8320 Val loss: 2.9447 Val Acc: 0.8664\n",
      "Epoch: 2 Train loss: 3.2295 Train Acc: 0.8503 Val loss: 3.0452 Val Acc: 0.8537\n",
      "Epoch: 3 Train loss: 2.7279 Train Acc: 0.8720 Val loss: 2.7827 Val Acc: 0.8726\n",
      "Epoch: 4 Train loss: 2.4835 Train Acc: 0.8826 Val loss: 2.8961 Val Acc: 0.8661\n",
      "Epoch: 5 Train loss: 2.3017 Train Acc: 0.8902 Val loss: 2.8072 Val Acc: 0.8730\n",
      "Epoch: 6 Train loss: 2.1768 Train Acc: 0.8965 Val loss: 3.0672 Val Acc: 0.8649\n",
      "Epoch: 7 Train loss: 2.0383 Train Acc: 0.9038 Val loss: 2.8482 Val Acc: 0.8743\n",
      "Epoch: 8 Train loss: 1.9319 Train Acc: 0.9070 Val loss: 2.7546 Val Acc: 0.8788\n",
      "Epoch: 9 Train loss: 1.8487 Train Acc: 0.9115 Val loss: 2.6805 Val Acc: 0.8817\n",
      "Epoch: 10 Train loss: 1.7777 Train Acc: 0.9155 Val loss: 2.6514 Val Acc: 0.8876\n",
      "Epoch: 11 Train loss: 1.7033 Train Acc: 0.9183 Val loss: 2.8771 Val Acc: 0.8812\n",
      "Epoch: 12 Train loss: 1.6482 Train Acc: 0.9204 Val loss: 2.6498 Val Acc: 0.8890\n",
      "Epoch: 13 Train loss: 1.5534 Train Acc: 0.9249 Val loss: 2.6844 Val Acc: 0.8892\n",
      "Epoch: 14 Train loss: 1.5034 Train Acc: 0.9266 Val loss: 2.7331 Val Acc: 0.8829\n",
      "Epoch: 15 Train loss: 1.4676 Train Acc: 0.9285 Val loss: 2.8622 Val Acc: 0.8882\n",
      "Epoch: 16 Train loss: 1.4116 Train Acc: 0.9319 Val loss: 3.0216 Val Acc: 0.8746\n",
      "Epoch: 17 Train loss: 1.3502 Train Acc: 0.9355 Val loss: 2.9299 Val Acc: 0.8876\n",
      "Epoch: 18 Train loss: 1.3281 Train Acc: 0.9352 Val loss: 2.8964 Val Acc: 0.8864\n",
      "Epoch: 19 Train loss: 1.2172 Train Acc: 0.9403 Val loss: 2.9031 Val Acc: 0.8890\n",
      "Epoch: 20 Train loss: 1.2087 Train Acc: 0.9412 Val loss: 4.1170 Val Acc: 0.8851\n",
      "\n",
      "Final - Best Train Acc: 0.9412 Best Val acc: 0.8892\n",
      "CPU times: user 4min 50s, sys: 360 ms, total: 4min 51s\n",
      "Wall time: 4min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Convert to pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = 0.05\n",
    "shape = (100, 100)\n",
    "# initial weights matrix\n",
    "A = torch.randn(shape) * 1e-2\n",
    "# sparse mask\n",
    "M = torch.rand(shape) < sparsity\n",
    "# apply sparse mask\n",
    "A = M.float() * A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1, dtype=torch.uint8), tensor(0.0082), tensor(0.0110), tensor(0.0037))"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the magnitude treshold\n",
    "def non_zero(tensor):\n",
    "    return tensor.view(-1)[tensor.view(-1).nonzero()].view(-1)\n",
    "\n",
    "n_total = np.prod(shape)\n",
    "n_zeros = torch.sum(A == 0).item()\n",
    "n_values = n_total - n_zeros\n",
    "zeta = 0.3\n",
    "A_abs = torch.abs(A)\n",
    "\n",
    "# threshold = torch.kthvalue(A.view(-1), n_values*zeta + n_zeros)\n",
    "threshold, _ = torch.kthvalue(non_zero(A_abs), int(zeta*(n_values)))\n",
    "N = A_abs > threshold.item()\n",
    "A_prime = A * N.float()\n",
    "\n",
    "# assure that the mean of absolute values has increased\n",
    "prev_mean = torch.mean(torch.abs(non_zero(A)))\n",
    "post_mean = torch.mean(torch.abs(non_zero(A_prime)))\n",
    "prev_mean < post_mean, prev_mean, post_mean , threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2955"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract mean and std of current weights\n",
    "# if batch norm, can also get from the batch norm layer the running mean and std\n",
    "A_nonzero = non_zero(A)\n",
    "prev_mean = torch.mean(A_nonzero)\n",
    "prev_std = torch.std(A_nonzero)\n",
    "\n",
    "# initialize matrix of random values \n",
    "R = torch.randn(shape) * prev_std + prev_mean \n",
    "# zero out values where there were previous weights\n",
    "R_prime = R * (M == 0).float()\n",
    "# decide which matrices to keep\n",
    "p_update = zeta / (1-sparsity)\n",
    "P = torch.rand(shape) < p_update\n",
    "# only keep weights selected\n",
    "R_double_prime = R_prime * P.float()\n",
    "\n",
    "# assure that approximately 30% of the weights are selected\n",
    "torch.sum(R_double_prime != 0).item() / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0003), tensor(0.0108))"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get final matrix \n",
    "A_new = A_prime + R_double_prime\n",
    "\n",
    "# test if mean and std remains the same as before\n",
    "A_nonzero = non_zero(A_new)\n",
    "mean = torch.mean(A_nonzero)\n",
    "std = torch.std(A_nonzero)\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Calculations on how to implement SET with vectorized operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = 0.05\n",
    "shape = (100, 100)\n",
    "# initial weights matrix\n",
    "A = np.random.normal(0, 0.1, shape)\n",
    "# sparse mask\n",
    "M = np.random.random(shape) < sparsity\n",
    "# apply sparse mask\n",
    "A = M * A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 0.07901503584891117, 0.10486261548644035, 0.04179074620889921)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the magnitude treshold\n",
    "zeta = 0.3\n",
    "A_abs = np.abs(A)\n",
    "threshold = np.quantile(np.extract(A_abs > 0, A_abs), zeta)\n",
    "N = A_abs > threshold\n",
    "A_prime = A * N\n",
    "\n",
    "# test that the mean of absolute values has increased\n",
    "prev_mean = np.mean(np.abs(np.extract(A != 0, A)))\n",
    "post_mean = np.mean(np.abs(np.extract(A_prime != 0, A_prime)))\n",
    "prev_mean < post_mean, prev_mean, post_mean , threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3012"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract mean and std of current weights\n",
    "# if batch norm, can also get from the batch norm layer the running mean and std\n",
    "A_nonzero = np.extract(A != 0, A)\n",
    "prev_mean = np.mean(A_nonzero)\n",
    "prev_std = np.std(A_nonzero)\n",
    "prev_mean, prev_std\n",
    "\n",
    "# initialize matrix of random values \n",
    "R = np.random.normal(prev_mean, prev_std, shape)\n",
    "# zero out values where there were previous weights\n",
    "R_prime = R * (M == 0)\n",
    "# decide which matrices to keep\n",
    "p_update = zeta / (1-sparsity)\n",
    "P = np.random.random(shape) < p_update\n",
    "# only keep weights selected\n",
    "R_double_prime = R_prime * P\n",
    "\n",
    "# test if approximately 30% of the weights are selected\n",
    "np.sum(R_double_prime != 0) / np.prod(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.003130835326136144, 0.10096178221531257)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get final matrix \n",
    "A_new = A_prime + R_double_prime\n",
    "\n",
    "# test if mean and std remains the same as before\n",
    "A_nonzero = np.extract(A_new != 0, A_new)\n",
    "mean = np.mean(A_nonzero)\n",
    "std = np.std(A_nonzero)\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Code \n",
    "#### (nothing useful beyond this point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3701715943170711e-05\n",
      "1.185009750770405e-06\n",
      "1.3731943909078836e-05\n",
      "4.772923603013624e-07\n",
      "8.516578418493737e-06\n",
      "2.6653063400772226e-07\n",
      "2.239644469881341e-09\n",
      "-3.5845091588271316e-06\n"
     ]
    }
   ],
   "source": [
    "# modules already have the right shape\n",
    "masks = iter(model.masks)\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Linear):\n",
    "        print(torch.mean(m.weight.grad).item())\n",
    "        \n",
    "        # apply the mask\n",
    "        m.weight.grad *= next(masks)\n",
    "        print(torch.mean(m.weight.grad).item())\n",
    "        \n",
    "        \n",
    "# these experiments show I wouldn't need to freeze 0 layers\n",
    "# need to understand the theory behind it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "        1., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[0, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + m.weight[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
       "         0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0063,  0.0000,  0.0000,\n",
       "         0.0037,  0.0000,  0.0156, -0.0000], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.weight[0, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1000])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__array__',\n",
       " '__array_priority__',\n",
       " '__array_wrap__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__cuda_array_interface__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__float__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__idiv__',\n",
       " '__ilshift__',\n",
       " '__imul__',\n",
       " '__index__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__int__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__irshift__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__long__',\n",
       " '__lshift__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rfloordiv__',\n",
       " '__rmul__',\n",
       " '__rpow__',\n",
       " '__rshift__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_backward_hooks',\n",
       " '_base',\n",
       " '_cdata',\n",
       " '_coalesced_',\n",
       " '_dimI',\n",
       " '_dimV',\n",
       " '_grad',\n",
       " '_grad_fn',\n",
       " '_indices',\n",
       " '_make_subclass',\n",
       " '_nnz',\n",
       " '_values',\n",
       " '_version',\n",
       " 'abs',\n",
       " 'abs_',\n",
       " 'acos',\n",
       " 'acos_',\n",
       " 'add',\n",
       " 'add_',\n",
       " 'addbmm',\n",
       " 'addbmm_',\n",
       " 'addcdiv',\n",
       " 'addcdiv_',\n",
       " 'addcmul',\n",
       " 'addcmul_',\n",
       " 'addmm',\n",
       " 'addmm_',\n",
       " 'addmv',\n",
       " 'addmv_',\n",
       " 'addr',\n",
       " 'addr_',\n",
       " 'all',\n",
       " 'allclose',\n",
       " 'any',\n",
       " 'apply_',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'as_strided',\n",
       " 'as_strided_',\n",
       " 'asin',\n",
       " 'asin_',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atan2_',\n",
       " 'atan_',\n",
       " 'backward',\n",
       " 'baddbmm',\n",
       " 'baddbmm_',\n",
       " 'bernoulli',\n",
       " 'bernoulli_',\n",
       " 'bincount',\n",
       " 'bmm',\n",
       " 'btrifact',\n",
       " 'btrifact_with_info',\n",
       " 'btrisolve',\n",
       " 'byte',\n",
       " 'cauchy_',\n",
       " 'ceil',\n",
       " 'ceil_',\n",
       " 'char',\n",
       " 'cholesky',\n",
       " 'chunk',\n",
       " 'clamp',\n",
       " 'clamp_',\n",
       " 'clamp_max',\n",
       " 'clamp_max_',\n",
       " 'clamp_min',\n",
       " 'clamp_min_',\n",
       " 'clone',\n",
       " 'coalesce',\n",
       " 'contiguous',\n",
       " 'copy_',\n",
       " 'cos',\n",
       " 'cos_',\n",
       " 'cosh',\n",
       " 'cosh_',\n",
       " 'cpu',\n",
       " 'cross',\n",
       " 'cuda',\n",
       " 'cumprod',\n",
       " 'cumsum',\n",
       " 'data',\n",
       " 'data_ptr',\n",
       " 'dense_dim',\n",
       " 'det',\n",
       " 'detach',\n",
       " 'detach_',\n",
       " 'device',\n",
       " 'diag',\n",
       " 'diag_embed',\n",
       " 'diagflat',\n",
       " 'diagonal',\n",
       " 'digamma',\n",
       " 'digamma_',\n",
       " 'dim',\n",
       " 'dist',\n",
       " 'div',\n",
       " 'div_',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'eig',\n",
       " 'element_size',\n",
       " 'eq',\n",
       " 'eq_',\n",
       " 'equal',\n",
       " 'erf',\n",
       " 'erf_',\n",
       " 'erfc',\n",
       " 'erfc_',\n",
       " 'erfinv',\n",
       " 'erfinv_',\n",
       " 'exp',\n",
       " 'exp_',\n",
       " 'expand',\n",
       " 'expand_as',\n",
       " 'expm1',\n",
       " 'expm1_',\n",
       " 'exponential_',\n",
       " 'fft',\n",
       " 'fill_',\n",
       " 'flatten',\n",
       " 'flip',\n",
       " 'float',\n",
       " 'floor',\n",
       " 'floor_',\n",
       " 'fmod',\n",
       " 'fmod_',\n",
       " 'frac',\n",
       " 'frac_',\n",
       " 'gather',\n",
       " 'ge',\n",
       " 'ge_',\n",
       " 'gels',\n",
       " 'geometric_',\n",
       " 'geqrf',\n",
       " 'ger',\n",
       " 'gesv',\n",
       " 'get_device',\n",
       " 'grad',\n",
       " 'grad_fn',\n",
       " 'gt',\n",
       " 'gt_',\n",
       " 'half',\n",
       " 'hardshrink',\n",
       " 'histc',\n",
       " 'ifft',\n",
       " 'index_add',\n",
       " 'index_add_',\n",
       " 'index_copy',\n",
       " 'index_copy_',\n",
       " 'index_fill',\n",
       " 'index_fill_',\n",
       " 'index_put',\n",
       " 'index_put_',\n",
       " 'index_select',\n",
       " 'indices',\n",
       " 'int',\n",
       " 'inverse',\n",
       " 'irfft',\n",
       " 'is_coalesced',\n",
       " 'is_complex',\n",
       " 'is_contiguous',\n",
       " 'is_cuda',\n",
       " 'is_distributed',\n",
       " 'is_floating_point',\n",
       " 'is_leaf',\n",
       " 'is_nonzero',\n",
       " 'is_pinned',\n",
       " 'is_same_size',\n",
       " 'is_set_to',\n",
       " 'is_shared',\n",
       " 'is_signed',\n",
       " 'is_sparse',\n",
       " 'isclose',\n",
       " 'item',\n",
       " 'kthvalue',\n",
       " 'layout',\n",
       " 'le',\n",
       " 'le_',\n",
       " 'lerp',\n",
       " 'lerp_',\n",
       " 'lgamma',\n",
       " 'lgamma_',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log10_',\n",
       " 'log1p',\n",
       " 'log1p_',\n",
       " 'log2',\n",
       " 'log2_',\n",
       " 'log_',\n",
       " 'log_normal_',\n",
       " 'log_softmax',\n",
       " 'logdet',\n",
       " 'logsumexp',\n",
       " 'long',\n",
       " 'lt',\n",
       " 'lt_',\n",
       " 'map2_',\n",
       " 'map_',\n",
       " 'masked_fill',\n",
       " 'masked_fill_',\n",
       " 'masked_scatter',\n",
       " 'masked_scatter_',\n",
       " 'masked_select',\n",
       " 'matmul',\n",
       " 'matrix_power',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'min',\n",
       " 'mm',\n",
       " 'mode',\n",
       " 'mul',\n",
       " 'mul_',\n",
       " 'multinomial',\n",
       " 'mv',\n",
       " 'mvlgamma',\n",
       " 'mvlgamma_',\n",
       " 'name',\n",
       " 'narrow',\n",
       " 'narrow_copy',\n",
       " 'ndimension',\n",
       " 'ne',\n",
       " 'ne_',\n",
       " 'neg',\n",
       " 'neg_',\n",
       " 'nelement',\n",
       " 'new',\n",
       " 'new_empty',\n",
       " 'new_full',\n",
       " 'new_ones',\n",
       " 'new_tensor',\n",
       " 'new_zeros',\n",
       " 'nonzero',\n",
       " 'norm',\n",
       " 'normal_',\n",
       " 'numel',\n",
       " 'numpy',\n",
       " 'orgqr',\n",
       " 'ormqr',\n",
       " 'output_nr',\n",
       " 'permute',\n",
       " 'pin_memory',\n",
       " 'pinverse',\n",
       " 'polygamma',\n",
       " 'polygamma_',\n",
       " 'potrf',\n",
       " 'potri',\n",
       " 'potrs',\n",
       " 'pow',\n",
       " 'pow_',\n",
       " 'prelu',\n",
       " 'prod',\n",
       " 'pstrf',\n",
       " 'put_',\n",
       " 'qr',\n",
       " 'random_',\n",
       " 'reciprocal',\n",
       " 'reciprocal_',\n",
       " 'record_stream',\n",
       " 'register_hook',\n",
       " 'reinforce',\n",
       " 'relu',\n",
       " 'relu_',\n",
       " 'remainder',\n",
       " 'remainder_',\n",
       " 'renorm',\n",
       " 'renorm_',\n",
       " 'repeat',\n",
       " 'requires_grad',\n",
       " 'requires_grad_',\n",
       " 'reshape',\n",
       " 'reshape_as',\n",
       " 'resize',\n",
       " 'resize_',\n",
       " 'resize_as',\n",
       " 'resize_as_',\n",
       " 'retain_grad',\n",
       " 'rfft',\n",
       " 'roll',\n",
       " 'rot90',\n",
       " 'round',\n",
       " 'round_',\n",
       " 'rsqrt',\n",
       " 'rsqrt_',\n",
       " 'scatter',\n",
       " 'scatter_',\n",
       " 'scatter_add',\n",
       " 'scatter_add_',\n",
       " 'select',\n",
       " 'set_',\n",
       " 'shape',\n",
       " 'share_memory_',\n",
       " 'short',\n",
       " 'sigmoid',\n",
       " 'sigmoid_',\n",
       " 'sign',\n",
       " 'sign_',\n",
       " 'sin',\n",
       " 'sin_',\n",
       " 'sinh',\n",
       " 'sinh_',\n",
       " 'size',\n",
       " 'slogdet',\n",
       " 'smm',\n",
       " 'softmax',\n",
       " 'sort',\n",
       " 'sparse_dim',\n",
       " 'sparse_mask',\n",
       " 'sparse_resize_',\n",
       " 'sparse_resize_and_clear_',\n",
       " 'split',\n",
       " 'split_with_sizes',\n",
       " 'sqrt',\n",
       " 'sqrt_',\n",
       " 'squeeze',\n",
       " 'squeeze_',\n",
       " 'sspaddmm',\n",
       " 'std',\n",
       " 'stft',\n",
       " 'storage',\n",
       " 'storage_offset',\n",
       " 'storage_type',\n",
       " 'stride',\n",
       " 'sub',\n",
       " 'sub_',\n",
       " 'sum',\n",
       " 'svd',\n",
       " 'symeig',\n",
       " 't',\n",
       " 't_',\n",
       " 'take',\n",
       " 'tan',\n",
       " 'tan_',\n",
       " 'tanh',\n",
       " 'tanh_',\n",
       " 'to',\n",
       " 'to_dense',\n",
       " 'to_sparse',\n",
       " 'tolist',\n",
       " 'topk',\n",
       " 'trace',\n",
       " 'transpose',\n",
       " 'transpose_',\n",
       " 'tril',\n",
       " 'tril_',\n",
       " 'triu',\n",
       " 'triu_',\n",
       " 'trtrs',\n",
       " 'trunc',\n",
       " 'trunc_',\n",
       " 'type',\n",
       " 'type_as',\n",
       " 'unbind',\n",
       " 'unfold',\n",
       " 'uniform_',\n",
       " 'unique',\n",
       " 'unsqueeze',\n",
       " 'unsqueeze_',\n",
       " 'values',\n",
       " 'var',\n",
       " 'view',\n",
       " 'view_as',\n",
       " 'where',\n",
       " 'zero_']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backend',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_name',\n",
       " '_initialize_weights',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_tracing_name',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'classifier',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'device',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'half',\n",
       " 'input_size',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
